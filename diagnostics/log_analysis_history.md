# ログ分析レポート: カリキュラム移行時の学習崩壊

## 1. ログの概要
提供された `checkpoints/history copy.csv` を分析しました。

*   **Epoch 1-259 (Phase 1相当?)**:
    *   Loss: 140.5 -> 0.013 (劇的に改善)
    *   CER: 0.81 -> 0.00 (完璧に学習)
    *   状態: 2文字 (K, M) の識別は完全に成功している。
*   **Epoch 260 (Phase Change?)**:
    *   Loss: 0.013 -> 2.66 (200倍に急増)
    *   CER: 0.00 -> 0.13
    *   状態: 新しい文字が追加された瞬間、モデルが混乱している。
*   **Epoch 260-330 (Phase 6相当?)**:
    *   Loss: 0.30 - 0.40 付近で停滞（高止まり）。
    *   CER: 0.17 - 0.20 付近で停滞。
    *   状態: **学習が進んでいない（Plateau）**。

## 2. 診断: なぜ学習が止まったのか？

Epoch 260 で何が起きたかを推測します。ユーザーのコメント「カリキュラム1でCERが0%までさがったので、カリキュラム6まですすめた」から、**一気に文字種を増やした（2文字 -> 12文字程度）** と考えられます。

### 仮説A: 急激な難易度上昇 (Curriculum Shock)
*   2文字 (K, M) の世界に過剰適応（Overfit）していたモデルに対し、いきなり10文字以上の未知の文字を投入したため、モデルの内部表現が破壊された（Catastrophic Forgetting）。
*   特に、K, M 以外の文字に対する特徴抽出器が育っていない状態で、CTCがパニックを起こしている（アライメントが見つからない）。

### 仮説B: 似た文字の混同 (Confusion)
*   Phase 6 までに進むと、以下のような似たリズムの文字が登場します。
    *   K (-.-) vs R (.-.)
    *   M (--) vs O (---) vs T (-)
*   これらを区別するには微細な長さの違いを見極める必要がありますが、モデルはこれまで「KかMか」という単純な二択に特化していたため、その微細な特徴を見ていません。

## 3. 対策方針

「一気に進める」のではなく、**「少しずつ広げる」** 戦略が必要です。

1.  **インクリメンタルな拡張**:
    *   Phase 1 (2文字) -> Phase 6 (12文字) と飛ばすのではなく、Phase 2 (4文字), Phase 3 (6文字)... と段階を踏む。
    *   `train.py` の自動カリキュラム機能 (`phase = (epoch + 1) // 2`) を活用するか、手動で細かく制御する。

2.  **学習率の再調整**:
    *   新しい文字が入った直後は、既存の知識を壊さないように学習率を少し下げるか、逆に未知の領域を探索するために Warmup をやり直す必要がある。
    *   現状のログでは `lr=0.0003` で固定されているが、Lossが跳ね上がった時は少し下げて安定させるのが定石。

3.  **過去の知識の維持 (Replay)**:
    *   新しい文字だけでなく、K, M も混ぜ続ける（現在のDataset実装はランダム選択なのでこれは満たされているはず）。

## 推奨アクション

ユーザーへの提案：
「一気に Phase 6 に進めるのではなく、**Phase 2, 3, 4... と順番に進めてください**。モデルは K/M の二択に特化しすぎており、急に多くの文字を見せられて混乱しています。」

システム側の修正案：
`train.py` のカリキュラムロジックにおいて、**「CERが十分に下がるまでは次のPhaseに進まない」** という自動制御（Adaptive Curriculum）を導入するのが理想的です。しかし、まずは手動で段階的に進めることを推奨します。