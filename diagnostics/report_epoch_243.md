# 学習停滞に関する診断レポート

## 1. 事実 (Facts)
ログおよびコード設定から確認された客観的事実は以下の通りです。

*   **Lossの停滞**: Epoch 241-243の間、Train Lossは 0.62-0.74、Val Lossは 0.62-0.65 で推移しており、劇的な改善が見られない。
*   **高いCER (Character Error Rate)**: Validation CER が 0.40-0.47 と高く、実用的なデコードができていない。
*   **極端なBlankバイアス**:
    *   ログ出力: `WARNING: Model is heavily biased towards BLANK.`
    *   Blankのロジット平均: 約 15.0
    *   文字クラスのロジット平均: 約 -1.9
    *   この差（約17.0）は、確率分布においてモデルが「ほぼ100%の確信を持ってBlank（無音）である」と判断していることを示す。
*   **物理パラメータ設定**:
    *   `HOP_LENGTH`: 160 (10ms)
    *   `SUBSAMPLING_RATE`: 2
    *   実効フレームレート: 20ms / frame
    *   WPM 20 における短点（Dot）の長さ: 約 60ms

## 2. 推測 (Hypotheses)
事実に基づき、以下のメカニズムで学習が阻害されていると推測されます。

*   **時間分解能の不足 (Critical)**:
    *   CWの短点（60ms）に対し、モデルの時間分解能（20ms）が粗すぎる。
    *   わずか3フレームで短点を表現する必要があり、位相のズレやフィルタリングの影響で信号の特徴が「ぼやけて」しまっている。
    *   結果として、モデルは信号のON/OFFを明確に区別できず、安全策として「ずっと無音（Blank）」を出力する局所解に陥っている。
*   **Blank Collapse (初期化の失敗)**:
    *   学習初期にモデルが「下手に文字を出力して間違えるより、何も出力しない方がLossが低い」という局所解（Local Minima）にハマっている。
    *   現在の `model.py` にある `self.ctc_head.bias[0] = -1.0` という抑制策では不十分であり、一度ついた約15.0という圧倒的な差を覆すことができていない。

## 3. 提案する対策 (Action Plan)
この状況を打破するために、以下の修正を提案します。

1.  **時間分解能の向上（最優先）**:
    *   `config.py`: `SUBSAMPLING_RATE` を `2` から `1` に変更（サブサンプリング廃止）。
    *   `config.py`: `HOP_LENGTH` を `160` から `128` (8ms) に短縮し、時間分解能を向上させる。
    *   これにより、短点1つあたり約7〜8フレームの情報量を確保し、信号検出を容易にする。

2.  **Blankバイアスの強制排除**:
    *   `model.py`: CTC Headの初期化において、Blankクラスのバイアスを `-1.0` から `-5.0` 程度まで強力に下げる。
    *   これにより、学習初期に「何か文字を出力すること」を強制し、アライメントの学習を促進する。

3.  **学習のリセット**:
    *   アーキテクチャ（時間次元）が変わるため、既存のチェックポイントは使用できない。
    *   `train.py` のカリキュラムを調整し、最初はさらに低速（10-15 WPM）かつ長めのテキストで、アライメントの学習を確実に成功させる。

この修正を行うには、`code` モードへの切り替えが必要です。